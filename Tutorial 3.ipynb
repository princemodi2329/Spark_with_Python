{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "|   Ploy|  35|      null| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|   null|  35|        15|  null|\n",
      "|   null|  65|        30|  null|\n",
      "|Randall|  30|        10| 70000|\n",
      "|  Bhumi|  29|      null| 60000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('test2.csv',header=True, inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  35|        15|100000|\n",
      "|  31|        11| 90000|\n",
      "|  35|      null| 90000|\n",
      "|null|         6| 85000|\n",
      "|  35|        15|  null|\n",
      "|  65|        30|  null|\n",
      "|  30|        10| 70000|\n",
      "|  29|      null| 60000|\n",
      "|null|         5| 70000|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = df_spark.drop('Name')\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "|   Ploy|  35|      null| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|   null|  35|        15|  null|\n",
      "|   null|  65|        30|  null|\n",
      "|Randall|  30|        10| 70000|\n",
      "|  Bhumi|  29|      null| 60000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('test2.csv',header=True, inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows based on null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "|   Ploy|  35|      null| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|   null|  35|        15|  null|\n",
      "|   null|  65|        30|  null|\n",
      "|Randall|  30|        10| 70000|\n",
      "|  Bhumi|  29|      null| 60000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how = (any / all)\n",
    "# If we select the parameter how = any, then if any of the row has even 1 null value, the whole row will be dropped.\n",
    "#If we select the parameter how = all, then the row will be dropped only if all the values are Null in that row\n",
    "\n",
    "df_spark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|    Ali| 35|        15|100000|\n",
      "| Prince| 31|        11| 90000|\n",
      "|Randall| 30|        10| 70000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "|   Ploy|  35|      null| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|Randall|  30|        10| 70000|\n",
      "|  Bhumi|  29|      null| 60000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Threshold - thresh\n",
    "# This parameter says that in order to delete a row, atleast thresh number of non-null values needs to be present.\n",
    "# In the below example, all the rows will be dropped that do not have atleast 3 non null values\n",
    "df_spark.na.drop(how='any', thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|   null|  35|        15|  null|\n",
      "|   null|  65|        30|  null|\n",
      "|Randall|  30|        10| 70000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset\n",
    "# Subset parameters allows us to select the column from which the null values should be dropped.\n",
    "# In the below example, all the rows that have null values in the Experience column will be dropped.\n",
    "\n",
    "df_spark.na.drop(how='any', subset = ['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|           Ali|  35|        15|100000|\n",
      "|        Prince|  31|        11| 90000|\n",
      "|          Ploy|  35|      null| 90000|\n",
      "|        Dipesh|null|         6| 85000|\n",
      "|Missing Values|  35|        15|  null|\n",
      "|Missing Values|  65|        30|  null|\n",
      "|       Randall|  30|        10| 70000|\n",
      "|         Bhumi|  29|      null| 60000|\n",
      "|        Laxita|null|         5| 70000|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|    Ali|  35|        15|100000|\n",
      "| Prince|  31|        11| 90000|\n",
      "|   Ploy|  35|      null| 90000|\n",
      "| Dipesh|null|         6| 85000|\n",
      "|  99999|  35|        15|  null|\n",
      "|  99999|  65|        30|  null|\n",
      "|Randall|  30|        10| 70000|\n",
      "|  Bhumi|  29|      null| 60000|\n",
      "| Laxita|null|         5| 70000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill('99999').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+----------+------+\n",
      "|          Name|  Age|Experience|Salary|\n",
      "+--------------+-----+----------+------+\n",
      "|           Ali|   35|        15|100000|\n",
      "|        Prince|   31|        11| 90000|\n",
      "|          Ploy|   35|    -99999| 90000|\n",
      "|        Dipesh|99999|         6| 85000|\n",
      "|Missing Values|   35|        15|     0|\n",
      "|Missing Values|   65|        30|     0|\n",
      "|       Randall|   30|        10| 70000|\n",
      "|         Bhumi|   29|    -99999| 60000|\n",
      "|        Laxita|99999|         5| 70000|\n",
      "+--------------+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill({'Name':'Missing Values', 'Age':99999, 'Experience':-99999, 'Salary':0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Null with Mean, Mode and Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace the values, we will need an Imputer function from spark package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Ali|  35|        15|100000|         35|                15|        100000|\n",
      "| Prince|  31|        11| 90000|         31|                11|         90000|\n",
      "|   Ploy|  35|      null| 90000|         35|                13|         90000|\n",
      "| Dipesh|null|         6| 85000|         37|                 6|         85000|\n",
      "|   null|  35|        15|  null|         35|                15|         80714|\n",
      "|   null|  65|        30|  null|         65|                30|         80714|\n",
      "|Randall|  30|        10| 70000|         30|                10|         70000|\n",
      "|  Bhumi|  29|      null| 60000|         29|                13|         60000|\n",
      "| Laxita|null|         5| 70000|         37|                 5|         70000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Ali|  35|        15|100000|         35|                15|        100000|\n",
      "| Prince|  31|        11| 90000|         31|                11|         90000|\n",
      "|   Ploy|  35|      null| 90000|         35|                11|         90000|\n",
      "| Dipesh|null|         6| 85000|         35|                 6|         85000|\n",
      "|   null|  35|        15|  null|         35|                15|         85000|\n",
      "|   null|  65|        30|  null|         65|                30|         85000|\n",
      "|Randall|  30|        10| 70000|         30|                10|         70000|\n",
      "|  Bhumi|  29|      null| 60000|         29|                11|         60000|\n",
      "| Laxita|null|         5| 70000|         35|                 5|         70000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    ").setStrategy('median')\n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Ali|  35|        15|100000|         35|                15|        100000|\n",
      "| Prince|  31|        11| 90000|         31|                11|         90000|\n",
      "|   Ploy|  35|      null| 90000|         35|                15|         90000|\n",
      "| Dipesh|null|         6| 85000|         35|                 6|         85000|\n",
      "|   null|  35|        15|  null|         35|                15|         70000|\n",
      "|   null|  65|        30|  null|         65|                30|         70000|\n",
      "|Randall|  30|        10| 70000|         30|                10|         70000|\n",
      "|  Bhumi|  29|      null| 60000|         29|                15|         60000|\n",
      "| Laxita|null|         5| 70000|         35|                 5|         70000|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    ").setStrategy('mode')\n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
